{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7468ef81",
   "metadata": {},
   "source": [
    "# Task 1: Manually complete a benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6ddca1",
   "metadata": {},
   "source": [
    "I choose a task with instance_id = 44. Below are the details of the task.\n",
    "\n",
    "**\"task_inst\"** (str): task goal description and output formatting instruction\n",
    "\n",
    "Analyze the inertial measurement unit (IMU) data collected during sleep and compute sleep endpoints. Load the given data and compute the following sleep endpoints: time of falling asleep, time of awakening, and total duration spent sleeping. The three values should be saved in a JSON file \"pred_results/imu_pred.json\", and the keys for them are \"sleep_onset\", \"wake_onset\", and \"total_sleep_duration\", respectively.\n",
    "\n",
    "**\"github_name\"** (str): the original github repository each task is adapted from\n",
    "\n",
    "mad-lab-fau/BioPsyKit\n",
    "\n",
    "**\"domain_knowledge\"** (str): expert-annotated information about the task\n",
    "\n",
    "Using the function sleep_processing_pipeline.predict_pipeline_acceleration() in BioPsyKit to perform the sleep processing pipeline. BioPsyKit is a Python package for the analysis of biopsychological data.\n",
    "\n",
    "**\"dataset_folder_tree\"** (str): string representation of dataset directory structure for each task\n",
    "\n",
    "|-- sleep_imu_data/   \n",
    "|---- sleep_data.pkl\n",
    "\n",
    "**\"src_file_or_path\" (str)**: source program location in the original github repository that is adapted\n",
    "\n",
    "examples/Sleep_IMU_Example.ipynb\n",
    "\n",
    "**\"gold_program_name\"**(str): name of annotated program (reference solution) for each task\n",
    "\n",
    "imu.py\n",
    "\n",
    "**\"output_fname\"** (str): output location to save the generated program for each task\n",
    "\n",
    "pred_results/imu_pred.json\n",
    "\n",
    "**\"dataset_preview\"** (str): string representation of the first few examples/lines in dataset files used in each task\n",
    "\n",
    "[START Preview of sleep_imu_data/sleep_data.pkl]\n",
    "time        gyr_x        gyr_y        gyr_z        acc_x        acc_y        acc_z\n",
    "2019-09-03 02:06:18.251953+02:00        -0.1220703125        -0.244140625        0.244140625        3.7553906250000004        -6.552773437500001        6.121669921875\n",
    "2019-09-03 02:06:18.256836+02:00        -0.42724609375        -0.1220703125        0.1220703125        3.7122802734375004        -6.5958837890625        6.140830078125\n",
    "2019-09-03 02:06:18.261718+02:00        -0.42724609375        -0.3662109375        0.18310546875        3.7122802734375004        -6.557563476562501        6.054609375\n",
    "...\n",
    "[END Preview of sleep_imu_data/sleep_data.pkl]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9305bd8e",
   "metadata": {},
   "source": [
    "When I get the task, I find the dataset to be analysed. Then I look up how to load a .pkl file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b42a5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load .pkl file\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d11198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'sleep_imu_data\\sleep_data.pkl'  # Replace with the actual path to your file\n",
    "with open(file_path, 'rb') as file:\n",
    "    # Load the data\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "840fecb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gyr_x</th>\n",
       "      <th>gyr_y</th>\n",
       "      <th>gyr_z</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-09-03 02:06:18.251953+02:00</th>\n",
       "      <td>-0.122070</td>\n",
       "      <td>-0.244141</td>\n",
       "      <td>0.244141</td>\n",
       "      <td>3.755391</td>\n",
       "      <td>-6.552773</td>\n",
       "      <td>6.121670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-03 02:06:18.256836+02:00</th>\n",
       "      <td>-0.427246</td>\n",
       "      <td>-0.122070</td>\n",
       "      <td>0.122070</td>\n",
       "      <td>3.712280</td>\n",
       "      <td>-6.595884</td>\n",
       "      <td>6.140830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-03 02:06:18.261718+02:00</th>\n",
       "      <td>-0.427246</td>\n",
       "      <td>-0.366211</td>\n",
       "      <td>0.183105</td>\n",
       "      <td>3.712280</td>\n",
       "      <td>-6.557563</td>\n",
       "      <td>6.054609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-03 02:06:18.266601+02:00</th>\n",
       "      <td>-0.488281</td>\n",
       "      <td>-0.183105</td>\n",
       "      <td>0.122070</td>\n",
       "      <td>3.750601</td>\n",
       "      <td>-6.567144</td>\n",
       "      <td>6.097720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-03 02:06:18.271484+02:00</th>\n",
       "      <td>-0.732422</td>\n",
       "      <td>0.244141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.731440</td>\n",
       "      <td>-6.547983</td>\n",
       "      <td>6.116880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     gyr_x     gyr_y     gyr_z     acc_x  \\\n",
       "time                                                                       \n",
       "2019-09-03 02:06:18.251953+02:00 -0.122070 -0.244141  0.244141  3.755391   \n",
       "2019-09-03 02:06:18.256836+02:00 -0.427246 -0.122070  0.122070  3.712280   \n",
       "2019-09-03 02:06:18.261718+02:00 -0.427246 -0.366211  0.183105  3.712280   \n",
       "2019-09-03 02:06:18.266601+02:00 -0.488281 -0.183105  0.122070  3.750601   \n",
       "2019-09-03 02:06:18.271484+02:00 -0.732422  0.244141  0.000000  3.731440   \n",
       "\n",
       "                                     acc_y     acc_z  \n",
       "time                                                  \n",
       "2019-09-03 02:06:18.251953+02:00 -6.552773  6.121670  \n",
       "2019-09-03 02:06:18.256836+02:00 -6.595884  6.140830  \n",
       "2019-09-03 02:06:18.261718+02:00 -6.557563  6.054609  \n",
       "2019-09-03 02:06:18.266601+02:00 -6.567144  6.097720  \n",
       "2019-09-03 02:06:18.271484+02:00 -6.547983  6.116880  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61366013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4048332, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeb94f2",
   "metadata": {},
   "source": [
    "When I look at this dataset, I have no idea what the meaning of each column is. So I search for the description os the inertial measurement unit (IMU) data. Wikipedia tells that an inertial measurement unit (IMU) is an electronic device that measures and reports a body's specific force, angular rate, and sometimes the orientation of the body, using a combination of accelerometers, gyroscopes, and sometimes magnetometers.   \n",
    "\n",
    "Then I search for how to handle IMU data in computing sleep endpoints. I find [this repo](https://github.com/mad-lab-fau/BioPsyKit) has a solution. The author create a Python package for the analysis of biopsychological data, including algorithms and data processing pipelines for sleep/wake prediction and computation of sleep endpoints based on activity or IMU data. So I pip install biopsykit.\n",
    "\n",
    "In the document, I also know that an inertial measurement unit (IMU) is a sensor that measures a body's acceleration (using accelerometers) and angular rate (using gyroscopes). In medical and psychological applications IMUs are commonly used for activity monitoring, movement analysis, and many more.\n",
    "\n",
    "Now I want to compute the following sleep endpoints: time of falling asleep, time of awakening, and total duration spent sleeping. By reading the document of biopsykit, I find the [Sleep_IMU_Example.ipynb](https://github.com/mad-lab-fau/BioPsyKit/blob/main/examples/Sleep_IMU_Example.ipynb), which contains detailed processing of IMU data to predict sleep/wake.\n",
    "\n",
    "Here is the code to complete the task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c02a910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install biopsykit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d476aaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Users\\Chenxi\\Documents\\GitHub\\AI_agent\\hw\\assignment-2-rag-Chenxi1818\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import biopsykit as bp\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9098e241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204.8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the fs value\n",
    "data_example, fs = bp.example_data.get_sleep_imu_example()\n",
    "fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28ed0bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleep_wake\n",
      "1.0           243\n",
      "0.0           152\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Compute features\n",
    "sleep_results = bp.sleep.sleep_processing_pipeline.predict_pipeline_acceleration(\n",
    "    data_example, sampling_rate=fs\n",
    ")\n",
    "sleep_endpoints = sleep_results[\"sleep_endpoints\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96df8d94",
   "metadata": {},
   "source": [
    "After run the example code given from the contributor of biopsykit, I think it would work for the data given by the author of ScienceAgentBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "567dd34c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfBoundsDatetime",
     "evalue": "cannot convert input with unit 'us'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOverflowError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/conversion.pyx:156\u001b[39m, in \u001b[36mpandas._libs.tslibs.conversion.cast_from_unit_vectorized\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mOverflowError\u001b[39m: value too large",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOutOfBoundsDatetime\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Users\\Chenxi\\Documents\\GitHub\\AI_agent\\hw\\assignment-2-rag-Chenxi1818\\.conda\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:517\u001b[39m, in \u001b[36m_to_datetime_with_unit\u001b[39m\u001b[34m(arg, unit, name, utc, errors)\u001b[39m\n\u001b[32m    516\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m     arr = \u001b[43mcast_from_unit_vectorized\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[43m=\u001b[49m\u001b[43munit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OutOfBoundsDatetime:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/conversion.pyx:160\u001b[39m, in \u001b[36mpandas._libs.tslibs.conversion.cast_from_unit_vectorized\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mOutOfBoundsDatetime\u001b[39m: cannot convert input 1.567469178251953e+18 with the unit 'us'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOutOfBoundsDatetime\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Compute features\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m sleep_results = \u001b[43mbp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep_processing_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_pipeline_acceleration\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfs\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m sleep_endpoints = sleep_results[\u001b[33m\"\u001b[39m\u001b[33msleep_endpoints\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Users\\Chenxi\\Documents\\GitHub\\AI_agent\\hw\\assignment-2-rag-Chenxi1818\\.conda\\Lib\\site-packages\\biopsykit\\sleep\\sleep_processing_pipeline\\sleep_processing_pipeline.py:83\u001b[39m, in \u001b[36mpredict_pipeline_acceleration\u001b[39m\u001b[34m(data, sampling_rate, convert_to_g, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m df_ac = accumulate_array(df_ac, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m / kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mepoch_length\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     82\u001b[39m df_sw = sw.predict(df_ac)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m df_rp = \u001b[43mrp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m bed_interval = [df_rp[\u001b[33m\"\u001b[39m\u001b[33mstart\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m], df_rp[\u001b[33m\"\u001b[39m\u001b[33mend\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m]]\n\u001b[32m     85\u001b[39m \u001b[38;5;28mprint\u001b[39m(df_sw.value_counts())\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Users\\Chenxi\\Documents\\GitHub\\AI_agent\\hw\\assignment-2-rag-Chenxi1818\\.conda\\Lib\\site-packages\\biopsykit\\signals\\imu\\rest_periods.py:74\u001b[39m, in \u001b[36mRestPeriods.predict\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m     68\u001b[39m overlap = \u001b[32m0\u001b[39m\n\u001b[32m     70\u001b[39m angle_sliding = sliding_window(\n\u001b[32m     71\u001b[39m     angle, window_sec=window_s, sampling_rate=\u001b[38;5;28mself\u001b[39m.sampling_rate, overlap_percent=overlap\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m index_resample = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_resample_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverlap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m df_angle = pd.DataFrame(\n\u001b[32m     77\u001b[39m     np.abs(np.diff(np.nanmean(angle_sliding, axis=\u001b[32m1\u001b[39m))), columns=[\u001b[33m\"\u001b[39m\u001b[33mangle\u001b[39m\u001b[33m\"\u001b[39m], index=index_resample[\u001b[32m1\u001b[39m:]\n\u001b[32m     78\u001b[39m )\n\u001b[32m     79\u001b[39m df_angle = df_angle.rolling(\u001b[32m60\u001b[39m).median()  \u001b[38;5;66;03m# rolling median, 60 * 5 seconds per sample = 5 minutes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Users\\Chenxi\\Documents\\GitHub\\AI_agent\\hw\\assignment-2-rag-Chenxi1818\\.conda\\Lib\\site-packages\\biopsykit\\signals\\imu\\rest_periods.py:130\u001b[39m, in \u001b[36mRestPeriods._resample_index\u001b[39m\u001b[34m(self, data, window_s, overlap)\u001b[39m\n\u001b[32m    125\u001b[39m index_resample = sliding_window(\n\u001b[32m    126\u001b[39m     data.index.values, window_sec=window_s, sampling_rate=\u001b[38;5;28mself\u001b[39m.sampling_rate, overlap_percent=overlap\n\u001b[32m    127\u001b[39m )[:, \u001b[32m0\u001b[39m]\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data.index, pd.DatetimeIndex):\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     index_resample = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_resample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mus\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m     index_resample = pd.DatetimeIndex(index_resample).tz_convert(data.index.tzinfo)\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m index_resample\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Users\\Chenxi\\Documents\\GitHub\\AI_agent\\hw\\assignment-2-rag-Chenxi1818\\.conda\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1104\u001b[39m, in \u001b[36mto_datetime\u001b[39m\u001b[34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[39m\n\u001b[32m   1102\u001b[39m         result = _convert_and_box_cache(argc, cache_array)\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m         result = \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43margc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1106\u001b[39m     result = convert_listlike(np.array([arg]), \u001b[38;5;28mformat\u001b[39m)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Users\\Chenxi\\Documents\\GitHub\\AI_agent\\hw\\assignment-2-rag-Chenxi1818\\.conda\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:409\u001b[39m, in \u001b[36m_convert_listlike_datetimes\u001b[39m\u001b[34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[39m\n\u001b[32m    407\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    408\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot specify both format and unit\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to_datetime_with_unit\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(arg, \u001b[33m\"\u001b[39m\u001b[33mndim\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m) > \u001b[32m1\u001b[39m:\n\u001b[32m    411\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    412\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33marg must be a string, datetime, list, tuple, 1-d array, or Series\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    413\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Users\\Chenxi\\Documents\\GitHub\\AI_agent\\hw\\assignment-2-rag-Chenxi1818\\.conda\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:523\u001b[39m, in \u001b[36m_to_datetime_with_unit\u001b[39m\u001b[34m(arg, unit, name, utc, errors)\u001b[39m\n\u001b[32m    519\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m errors != \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    520\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m _to_datetime_with_unit(\n\u001b[32m    521\u001b[39m                 arg.astype(\u001b[38;5;28mobject\u001b[39m), unit, name, utc, errors\n\u001b[32m    522\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m OutOfBoundsDatetime(\n\u001b[32m    524\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcannot convert input with unit \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    525\u001b[39m         )\n\u001b[32m    527\u001b[39m arr = arr.view(\u001b[33m\"\u001b[39m\u001b[33mM8[ns]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    528\u001b[39m tz_parsed = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mOutOfBoundsDatetime\u001b[39m: cannot convert input with unit 'us'"
     ]
    }
   ],
   "source": [
    "# Compute features\n",
    "sleep_results = bp.sleep.sleep_processing_pipeline.predict_pipeline_acceleration(\n",
    "    data, sampling_rate=fs\n",
    ")\n",
    "sleep_endpoints = sleep_results[\"sleep_endpoints\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d19c7f6",
   "metadata": {},
   "source": [
    "Then I encounter an error. It seems that something wrong in the unit of input data. So I check if there exists difference in the datetime index format between the data and data_example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be4aa10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2019-09-03 01:01:12.001953+02:00',\n",
       "               '2019-09-03 01:01:12.006836+02:00',\n",
       "               '2019-09-03 01:01:12.011718+02:00',\n",
       "               '2019-09-03 01:01:12.016601+02:00',\n",
       "               '2019-09-03 01:01:12.021484+02:00',\n",
       "               '2019-09-03 01:01:12.026367+02:00',\n",
       "               '2019-09-03 01:01:12.031250+02:00',\n",
       "               '2019-09-03 01:01:12.036132+02:00',\n",
       "               '2019-09-03 01:01:12.041015+02:00',\n",
       "               '2019-09-03 01:01:12.045898+02:00',\n",
       "               ...\n",
       "               '2019-09-03 07:35:45.449218+02:00',\n",
       "               '2019-09-03 07:35:45.454101+02:00',\n",
       "               '2019-09-03 07:35:45.458984+02:00',\n",
       "               '2019-09-03 07:35:45.463867+02:00',\n",
       "               '2019-09-03 07:35:45.468750+02:00',\n",
       "               '2019-09-03 07:35:45.473632+02:00',\n",
       "               '2019-09-03 07:35:45.478515+02:00',\n",
       "               '2019-09-03 07:35:45.483398+02:00',\n",
       "               '2019-09-03 07:35:45.488281+02:00',\n",
       "               '2019-09-03 07:35:45.493164+02:00'],\n",
       "              dtype='datetime64[us, Europe/Berlin]', name='time', length=4848332, freq=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_example.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3bef647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2019-09-03 02:06:18.251953+02:00',\n",
       "               '2019-09-03 02:06:18.256836+02:00',\n",
       "               '2019-09-03 02:06:18.261718+02:00',\n",
       "               '2019-09-03 02:06:18.266601+02:00',\n",
       "               '2019-09-03 02:06:18.271484+02:00',\n",
       "               '2019-09-03 02:06:18.276367+02:00',\n",
       "               '2019-09-03 02:06:18.281250+02:00',\n",
       "               '2019-09-03 02:06:18.286132+02:00',\n",
       "               '2019-09-03 02:06:18.291015+02:00',\n",
       "               '2019-09-03 02:06:18.295898+02:00',\n",
       "               ...\n",
       "               '2019-09-03 07:35:45.449218+02:00',\n",
       "               '2019-09-03 07:35:45.454101+02:00',\n",
       "               '2019-09-03 07:35:45.458984+02:00',\n",
       "               '2019-09-03 07:35:45.463867+02:00',\n",
       "               '2019-09-03 07:35:45.468750+02:00',\n",
       "               '2019-09-03 07:35:45.473632+02:00',\n",
       "               '2019-09-03 07:35:45.478515+02:00',\n",
       "               '2019-09-03 07:35:45.483398+02:00',\n",
       "               '2019-09-03 07:35:45.488281+02:00',\n",
       "               '2019-09-03 07:35:45.493164+02:00'],\n",
       "              dtype='datetime64[ns, Europe/Berlin]', name='time', length=4048332, freq=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135d6d34",
   "metadata": {},
   "source": [
    "So the problem lies in the fact that the units of the index in the DataFrame are not in microseconds.\n",
    "\n",
    "Then, I search for \"DatetimeIndex unit change\" in Google. Below is how to convert the DatetimeIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68da43ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index = data.index.as_unit('us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce38ebb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2019-09-03 02:06:18.251953+02:00',\n",
       "               '2019-09-03 02:06:18.256836+02:00',\n",
       "               '2019-09-03 02:06:18.261718+02:00',\n",
       "               '2019-09-03 02:06:18.266601+02:00',\n",
       "               '2019-09-03 02:06:18.271484+02:00',\n",
       "               '2019-09-03 02:06:18.276367+02:00',\n",
       "               '2019-09-03 02:06:18.281250+02:00',\n",
       "               '2019-09-03 02:06:18.286132+02:00',\n",
       "               '2019-09-03 02:06:18.291015+02:00',\n",
       "               '2019-09-03 02:06:18.295898+02:00',\n",
       "               ...\n",
       "               '2019-09-03 07:35:45.449218+02:00',\n",
       "               '2019-09-03 07:35:45.454101+02:00',\n",
       "               '2019-09-03 07:35:45.458984+02:00',\n",
       "               '2019-09-03 07:35:45.463867+02:00',\n",
       "               '2019-09-03 07:35:45.468750+02:00',\n",
       "               '2019-09-03 07:35:45.473632+02:00',\n",
       "               '2019-09-03 07:35:45.478515+02:00',\n",
       "               '2019-09-03 07:35:45.483398+02:00',\n",
       "               '2019-09-03 07:35:45.488281+02:00',\n",
       "               '2019-09-03 07:35:45.493164+02:00'],\n",
       "              dtype='datetime64[us, Europe/Berlin]', name='time', length=4048332, freq=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb8b80b",
   "metadata": {},
   "source": [
    "Then I run the pipeline to see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f7a1d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleep_wake\n",
      "1.0           188\n",
      "0.0           142\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Compute features\n",
    "sleep_results = bp.sleep.sleep_processing_pipeline.predict_pipeline_acceleration(\n",
    "    data, sampling_rate=fs\n",
    ")\n",
    "sleep_endpoints = sleep_results[\"sleep_endpoints\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1755dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "        \"sleep_onset\": sleep_endpoints[\"sleep_onset\"],\n",
    "        \"wake_onset\": sleep_endpoints[\"wake_onset\"],\n",
    "        \"total_sleep_duration\": sleep_endpoints[\"total_sleep_duration\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be4a99e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b193884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder if it doesn't exist\n",
    "os.makedirs(\"pred_results\", exist_ok=True)\n",
    "\n",
    "with open(\"pred_results/imu_pred.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ca8fc9",
   "metadata": {},
   "source": [
    "The total time taken was about 90 minutes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
